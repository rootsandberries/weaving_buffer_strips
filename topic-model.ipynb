{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Models for Buffer Strip Research\n",
    "\n",
    "In this script we run some topic models on the dataset and view the results in a couple of ways\n",
    "\n",
    "Parts of this script are based on the sklearn topic model example found at https://scikit-learn.org/stable/auto_examples/applications/plot_topics_extraction_with_nmf_lda.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First we load the dataset and do some cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1019, 16)\n",
      "Index(['ID', 'Zotero_Key', 'Item_Type', 'Pub_Year', 'Authors', 'Authors_Clean',\n",
      "       'Title', 'Source', 'ISBN', 'ISSN', 'DOI', 'Url', 'Abstract', 'Pages',\n",
      "       'Volume', 'Manual_Tags'],\n",
      "      dtype='object')\n",
      "Removing 203 with missing Abstract\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Zotero_Key</th>\n",
       "      <th>Item_Type</th>\n",
       "      <th>Pub_Year</th>\n",
       "      <th>Authors</th>\n",
       "      <th>Authors_Clean</th>\n",
       "      <th>Title</th>\n",
       "      <th>Source</th>\n",
       "      <th>ISBN</th>\n",
       "      <th>ISSN</th>\n",
       "      <th>DOI</th>\n",
       "      <th>Url</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>Pages</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Manual_Tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>590</td>\n",
       "      <td>WC2BD8TZ</td>\n",
       "      <td>journalArticle</td>\n",
       "      <td>2004</td>\n",
       "      <td>MacLeod, A.|Wratten, S. D.|Sotherton, N. W.|Th...</td>\n",
       "      <td>MacLeod A.|Wratten S. D.|Sotherton N. W.|Thoma...</td>\n",
       "      <td>'beetle banks' as refuges for beneficial arthr...</td>\n",
       "      <td>Agricultural and Forest Entomology</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1461-9555</td>\n",
       "      <td>10.1111/j.1461-9563.2004.00215.x</td>\n",
       "      <td>://WOS:000221413300008</td>\n",
       "      <td>1 Significant differences in the overwintering...</td>\n",
       "      <td>147-154</td>\n",
       "      <td>6</td>\n",
       "      <td>eppi-reviewer4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>895</td>\n",
       "      <td>2CJAYP5V</td>\n",
       "      <td>journalArticle</td>\n",
       "      <td>2015</td>\n",
       "      <td>Szajdak Lech, Wojciech|Maryganova, Victoria|Sk...</td>\n",
       "      <td>Szajdak L. W.|Maryganova V.|Skakovskii E.|Tych...</td>\n",
       "      <td>1h and 13c nmr spectroscopic studies of hexane...</td>\n",
       "      <td>Chemosphere</td>\n",
       "      <td>NaN</td>\n",
       "      <td>456535</td>\n",
       "      <td>10.1016/j.chemosphere.2014.10.032</td>\n",
       "      <td>https://ezp.sub.su.se/login?url=http://search....</td>\n",
       "      <td>Comparative study of the composition of lipids...</td>\n",
       "      <td>1422-1427</td>\n",
       "      <td>119</td>\n",
       "      <td>eppi-reviewer4; WINDBREAKS, shelterbelts, etc....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>577</td>\n",
       "      <td>UWDGTDQZ</td>\n",
       "      <td>journalArticle</td>\n",
       "      <td>2006</td>\n",
       "      <td>Ludy, C.|Lang, A.</td>\n",
       "      <td>Ludy C.|Lang A.</td>\n",
       "      <td>a 3-year field-scale monitoring of foliage-dwe...</td>\n",
       "      <td>Biological Control</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10499644</td>\n",
       "      <td>10.1016/j.biocontrol.2006.05.010</td>\n",
       "      <td>https://ezp.sub.su.se/login?url=http://search....</td>\n",
       "      <td>Concerns have been raised that genetically mo...</td>\n",
       "      <td>314-324</td>\n",
       "      <td>38</td>\n",
       "      <td>GERMANY; eppi-reviewer4; Arthropod predators; ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>183</td>\n",
       "      <td>2QCB9ZPL</td>\n",
       "      <td>journalArticle</td>\n",
       "      <td>2003</td>\n",
       "      <td>Collins, K. L.|Boatman, N. D.|Wilcox, A.|Holla...</td>\n",
       "      <td>Collins K. L.|Boatman N. D.|Wilcox A.|Holland ...</td>\n",
       "      <td>a 5-year comparison of overwintering polyphago...</td>\n",
       "      <td>Annals of Applied Biology</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mar-46</td>\n",
       "      <td>10.1111/j.1744-7348.2003.00063.x</td>\n",
       "      <td>://WOS:000184640600008</td>\n",
       "      <td>Overwintering polyphagous predator density and...</td>\n",
       "      <td>63-71</td>\n",
       "      <td>143</td>\n",
       "      <td>eppi-reviewer4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>209</td>\n",
       "      <td>3Q8IHLK6</td>\n",
       "      <td>journalArticle</td>\n",
       "      <td>2002</td>\n",
       "      <td>Croxton, P. J.|Carvell, C.|Mountford, J. O.|Sp...</td>\n",
       "      <td>Croxton P. J.|Carvell C.|Mountford J. O.|Spark...</td>\n",
       "      <td>a comparison of green lanes and field margins ...</td>\n",
       "      <td>Biological Conservation</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.1016/S0006-3207(02)00074-5</td>\n",
       "      <td>http://www.scopus.com/inward/record.url?eid=2-...</td>\n",
       "      <td>There have been major changes in agricultural ...</td>\n",
       "      <td>365-374</td>\n",
       "      <td>107</td>\n",
       "      <td>Bees; eppi-reviewer4; Hedgerows; Farmland; Fie...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ID Zotero_Key       Item_Type  Pub_Year  \\\n",
       "0  590   WC2BD8TZ  journalArticle      2004   \n",
       "1  895   2CJAYP5V  journalArticle      2015   \n",
       "2  577   UWDGTDQZ  journalArticle      2006   \n",
       "3  183   2QCB9ZPL  journalArticle      2003   \n",
       "4  209   3Q8IHLK6  journalArticle      2002   \n",
       "\n",
       "                                             Authors  \\\n",
       "0  MacLeod, A.|Wratten, S. D.|Sotherton, N. W.|Th...   \n",
       "1  Szajdak Lech, Wojciech|Maryganova, Victoria|Sk...   \n",
       "2                                  Ludy, C.|Lang, A.   \n",
       "3  Collins, K. L.|Boatman, N. D.|Wilcox, A.|Holla...   \n",
       "4  Croxton, P. J.|Carvell, C.|Mountford, J. O.|Sp...   \n",
       "\n",
       "                                       Authors_Clean  \\\n",
       "0  MacLeod A.|Wratten S. D.|Sotherton N. W.|Thoma...   \n",
       "1  Szajdak L. W.|Maryganova V.|Skakovskii E.|Tych...   \n",
       "2                                    Ludy C.|Lang A.   \n",
       "3  Collins K. L.|Boatman N. D.|Wilcox A.|Holland ...   \n",
       "4  Croxton P. J.|Carvell C.|Mountford J. O.|Spark...   \n",
       "\n",
       "                                               Title  \\\n",
       "0  'beetle banks' as refuges for beneficial arthr...   \n",
       "1  1h and 13c nmr spectroscopic studies of hexane...   \n",
       "2  a 3-year field-scale monitoring of foliage-dwe...   \n",
       "3  a 5-year comparison of overwintering polyphago...   \n",
       "4  a comparison of green lanes and field margins ...   \n",
       "\n",
       "                               Source ISBN       ISSN  \\\n",
       "0  Agricultural and Forest Entomology  NaN  1461-9555   \n",
       "1                         Chemosphere  NaN     456535   \n",
       "2                  Biological Control  NaN   10499644   \n",
       "3           Annals of Applied Biology  NaN     Mar-46   \n",
       "4             Biological Conservation  NaN        NaN   \n",
       "\n",
       "                                 DOI  \\\n",
       "0   10.1111/j.1461-9563.2004.00215.x   \n",
       "1  10.1016/j.chemosphere.2014.10.032   \n",
       "2   10.1016/j.biocontrol.2006.05.010   \n",
       "3   10.1111/j.1744-7348.2003.00063.x   \n",
       "4      10.1016/S0006-3207(02)00074-5   \n",
       "\n",
       "                                                 Url  \\\n",
       "0                             ://WOS:000221413300008   \n",
       "1  https://ezp.sub.su.se/login?url=http://search....   \n",
       "2  https://ezp.sub.su.se/login?url=http://search....   \n",
       "3                             ://WOS:000184640600008   \n",
       "4  http://www.scopus.com/inward/record.url?eid=2-...   \n",
       "\n",
       "                                            Abstract      Pages Volume  \\\n",
       "0  1 Significant differences in the overwintering...    147-154      6   \n",
       "1  Comparative study of the composition of lipids...  1422-1427    119   \n",
       "2   Concerns have been raised that genetically mo...    314-324     38   \n",
       "3  Overwintering polyphagous predator density and...      63-71    143   \n",
       "4  There have been major changes in agricultural ...    365-374    107   \n",
       "\n",
       "                                         Manual_Tags  \n",
       "0                                     eppi-reviewer4  \n",
       "1  eppi-reviewer4; WINDBREAKS, shelterbelts, etc....  \n",
       "2  GERMANY; eppi-reviewer4; Arthropod predators; ...  \n",
       "3                                     eppi-reviewer4  \n",
       "4  Bees; eppi-reviewer4; Hedgerows; Farmland; Fie...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/20201215_EGM_Net_all-articles_clean.csv', encoding= 'unicode_escape')\n",
    "print(df.shape)\n",
    "print(df.columns)\n",
    "\n",
    "missings = df[pd.isna(df['Abstract'])].index\n",
    "print(f\"Removing {missings.shape[0]} with missing Abstract\")\n",
    "df = df[~df.index.isin(missings)].reset_index(drop=True)\n",
    "\n",
    "# clean the abstract text to remove copyright and other procedural parts\n",
    "df['Abstract'] = df['Abstract'].str.replace(\"(\\(C).*\",\"\",regex=True)\n",
    "df['Abstract'] = df['Abstract'].str.replace(\"(\\[C).*\",\"\",regex=True)\n",
    "df['Abstract'] = df['Abstract'].str.replace(\"(Elsevier Science).*\",\"\",regex=True)\n",
    "df['Abstract'] = df['Abstract'].str.replace(\"^Abstract:\",\"\",regex=True)\n",
    "df['Abstract'] = df['Abstract'].str.replace(\"(\\[ABSTRACT).*\",\"\",regex=True)\n",
    "\n",
    "\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we tokenize the abstracts, creating a matrix of documents and the scores for each feature in our vocabulary\n",
    "\n",
    "Tfidf vectorization weights features according to how infrequent they are in the dataset, so that common features have a smaller influence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "# Extracting tf-idf features for NMF\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_df=0.95, min_df=2, stop_words=\"english\"\n",
    ")\n",
    "tfidf = tfidf_vectorizer.fit_transform(df['Abstract'])\n",
    "\n",
    "\n",
    "# Use tf (raw term count) features for LDA.\n",
    "tf_vectorizer = CountVectorizer(\n",
    "    max_df=0.95, min_df=2, stop_words=\"english\"\n",
    ")\n",
    "tf = tf_vectorizer.fit_transform(df['Abstract'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's a function to plot the results of a topic model, it returns a list of titles which are simply the top 3 words of each topic \n",
    "\n",
    "def plot_top_words(model, feature_names, n_top_words, title, n_components):\n",
    "    fig, axes = plt.subplots(n_components//5,5, figsize=(30, 15), sharex=True)\n",
    "    axes = axes.flatten()\n",
    "    t_titles = []\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_features_ind = topic.argsort()[: -n_top_words - 1 : -1]\n",
    "        top_features = [feature_names[i] for i in top_features_ind]\n",
    "        weights = topic[top_features_ind]\n",
    "\n",
    "        ax = axes[topic_idx]\n",
    "        ax.barh(top_features, weights, height=0.7)\n",
    "        ax.set_title(f\"Topic {topic_idx}\", fontdict={\"fontsize\": 30})\n",
    "        ax.invert_yaxis()\n",
    "        ax.tick_params(axis=\"both\", which=\"major\", labelsize=20)\n",
    "        for i in \"top right left\".split():\n",
    "            ax.spines[i].set_visible(False)\n",
    "        fig.suptitle(title, fontsize=40)\n",
    "        \n",
    "        t_titles.append(\", \".join(top_features[:3]))\n",
    "\n",
    "    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    return t_titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below we run our first topic model and view the terms associated with each topic\n",
    "\n",
    "we can alter the number of topics by setting the `n_components` variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/max/software/py39/lib/python3.9/site-packages/sklearn/decomposition/_nmf.py:1422: FutureWarning: `alpha` was deprecated in version 1.0 and will be removed in 1.2. Use `alpha_W` and `alpha_H` instead\n",
      "  warnings.warn(\n",
      "/home/max/software/py39/lib/python3.9/site-packages/sklearn/decomposition/_nmf.py:289: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Mismatch between array dtype ('object') and format specifier ('%.18e')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/software/py39/lib/python3.9/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36msavetxt\u001b[0;34m(fname, X, fmt, delimiter, newline, header, footer, comments, encoding)\u001b[0m\n\u001b[1;32m   1432\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1433\u001b[0;31m                     \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1434\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: must be real number, not str",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_30788/2671154638.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavetxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"tm-results/nmf_W_{n_components}.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavetxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"tm-results/nmf_H_{n_components}.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnmf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomponents_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavetxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"tm-results/features_{n_components}.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtfidf_feature_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Now we plot the topic words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36msavetxt\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/software/py39/lib/python3.9/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36msavetxt\u001b[0;34m(fname, X, fmt, delimiter, newline, header, footer, comments, encoding)\u001b[0m\n\u001b[1;32m   1433\u001b[0m                     \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1434\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1435\u001b[0;31m                     raise TypeError(\"Mismatch between array dtype ('%s') and \"\n\u001b[0m\u001b[1;32m   1436\u001b[0m                                     \u001b[0;34m\"format specifier ('%s')\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1437\u001b[0m                                     % (str(X.dtype), format)) from e\n",
      "\u001b[0;31mTypeError\u001b[0m: Mismatch between array dtype ('object') and format specifier ('%.18e')"
     ]
    }
   ],
   "source": [
    "n_components = 15\n",
    "n_top_words = 10\n",
    "\n",
    "# He we define our model and its parameters\n",
    "nmf = NMF(n_components=n_components, random_state=1, alpha=0.1, l1_ratio=0.5)\n",
    "\n",
    "# Now we fit the model on the data and return the document-topic matrix we call W\n",
    "W = nmf.fit_transform(tfidf)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# We save the results as csvs\n",
    "np.savetxt(f\"tm-results/nmf_W_{n_components}.csv\", W, delimiter=\",\")\n",
    "np.savetxt(f\"tm-results/nmf_H_{n_components}.csv\", nmf.components_, delimiter=\",\")\n",
    "np.savetxt(f\"tm-results/features_{n_components}.csv\", tfidf_feature_names, delimiter = \",\")\n",
    "\n",
    "# Now we plot the topic words\n",
    "t_titles = plot_top_words(\n",
    "    nmf, tfidf_feature_names, n_top_words, \"Topics in NMF model (Frobenius norm)\", n_components\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['000', '0001', '001', ..., 'ûò27', 'ûò3', 'ûò5'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's a quick function to get the documents most associated with a given topic\n",
    "\n",
    "def get_topic_docs(W, t_index, n_docs):\n",
    "    dids = np.argsort(W[t_index,:])[::-1][:n_docs]\n",
    "    for did in dids:\n",
    "        print()\n",
    "        print(df['Abstract'][did])\n",
    "        \n",
    "# This prints the 1 document most associated with topic 5\n",
    "get_topic_docs(W, 5, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality reduction with UMAP\n",
    "\n",
    "Our doc-topic matrix is still hard to plot because it has `n_components` dimensions. We can use UMAP to reduce this to two dimensions, while maintaining the local and (to some extent) global structure of the multidimensional data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "print(W.shape)\n",
    "reducer = umap.UMAP()\n",
    "embedding = reducer.fit_transform(W)\n",
    "print(embedding.shape)\n",
    "fig, ax = plt.subplots(figsize=(7,5.7),dpi=125)\n",
    "\n",
    "ax.scatter(\n",
    "    embedding[:,0], embedding[:,1], \n",
    "    c=df.Pub_Year, cmap='YlGn', \n",
    "    alpha=0.7, s=8,\n",
    "    lw=0.5, ec=\"grey\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we want to plot this, add topic labels, and use some other aspect of the data to decide the colour of the points\n",
    "\n",
    "# We import a couple more libraries\n",
    "from sklearn.cluster import DBSCAN\n",
    "from scipy.spatial import ConvexHull\n",
    "import mpld3\n",
    "from adjustText import adjust_text\n",
    "\n",
    "# Initiate a figure object\n",
    "fig, ax = plt.subplots(figsize=(7,5.7),dpi=150)\n",
    "\n",
    "# Make a scatter plot with the dimensions of the embedding as X and Y arguments, and the publication year as the colour\n",
    "scatter = ax.scatter(\n",
    "    embedding[:,0], embedding[:,1], \n",
    "    c=df.Pub_Year, cmap='YlGn', \n",
    "    alpha=0.7, s=8,\n",
    "    lw=0.5, ec=\"grey\"\n",
    ")\n",
    "\n",
    "# This function takes a set of points associated with a given topic label, and adds that label in the center of each cluster of points that are found\n",
    "def cluster_label_points(\n",
    "    title, points, ax, eps,\n",
    "    min_cluster, clabel_size,\n",
    "    ):\n",
    "    \n",
    "    # cluster the points and get the cluster numbers of the points\n",
    "    db = DBSCAN(eps=eps,min_samples=min_cluster).fit(points)\n",
    "    labels = db.labels_\n",
    "    texts = []\n",
    "    \n",
    "    # For each cluster number\n",
    "    for l in set(labels):\n",
    "        # ignore the -1 cluster which is the remainder which cannot be clustered\n",
    "        if l==-1:\n",
    "            continue\n",
    "        \n",
    "        # get the indices of the points which have this cluster label\n",
    "        ind = np.argwhere(labels==l).ravel()\n",
    "        # The label points are those points with those indices\n",
    "        lpoints = points[ind]\n",
    "        # As long as the cluster is bigger than the min_cluster parameter, add a label\n",
    "        if len(ind) > min_cluster:\n",
    "            # Get the smallest shape that can be drawn around the point\n",
    "            hull = ConvexHull(lpoints)\n",
    "            # Get the center of that shape\n",
    "            cx = np.mean(hull.points[hull.vertices,0])\n",
    "            cy = np.mean(hull.points[hull.vertices,1])\n",
    "            c = [cx,cy]\n",
    "            # Get a short form of the title (just the first term)\n",
    "            title = title.split(\",\")[0].replace(\"{\",\"\")\n",
    "            # Add the label to the plot\n",
    "            text = ax.annotate(\n",
    "                title, c, fontsize=clabel_size,\n",
    "                ha=\"center\",va=\"center\",\n",
    "                bbox={'facecolor':\"white\", 'alpha':0.4, 'pad':0.2, 'boxstyle': 'round'}\n",
    "\n",
    "            )\n",
    "            texts.append(text)\n",
    "    return texts\n",
    "            \n",
    "# Set a couple of parameters for the plotting\n",
    "min_cluster = 5\n",
    "# eps is a parameter for dbscan: the maximum distance between two samples for them to be considered part of the same neighbourhood\n",
    "eps = 1\n",
    "# The font size for the cluster label\n",
    "clabel_size = 12\n",
    "# The quantile of doctopic scores for documents to be considered part of a topic\n",
    "t_thresh = 0.8\n",
    "\n",
    "# start an empty list of labels, so we can run adjust_text to prevent them overlapping\n",
    "texts = []\n",
    "\n",
    "# for each topic\n",
    "for t_index, title in enumerate(t_titles):\n",
    "    # Get the nonzero topic scores\n",
    "    scores = W[:,t_index][W[:,t_index].nonzero()]\n",
    "    # then get the threshold from those scores given the quantile parameter set above \n",
    "    thresh = np.quantile(scores, t_thresh)\n",
    "    # Get the indices of the documents above that threshold\n",
    "    highlight_docs = np.argwhere(W[:,t_index]>thresh).ravel()\n",
    "    # Get the points with those indices\n",
    "    points = embedding[highlight_docs]\n",
    "    # label the topic using the function defined above\n",
    "    texts += cluster_label_points(\n",
    "        title,\n",
    "        points,\n",
    "        ax,\n",
    "        eps,\n",
    "        min_cluster,\n",
    "        clabel_size\n",
    "    )\n",
    "    \n",
    "# Adjust the texts to prevent overlap\n",
    "adjust_text(texts,ax=ax, arrowprops=dict(arrowstyle=\"->\", color='None', lw=0.5))\n",
    "\n",
    "    \n",
    "# add gridlines  \n",
    "ax.grid(linestyle='-')\n",
    "# set x and y to be equal sizes\n",
    "ax.axis('equal')\n",
    "\n",
    "# Make a quick interactive version and save that to html\n",
    "tooltip = mpld3.plugins.PointLabelTooltip(scatter, labels=list(df['Title']))\n",
    "mpld3.plugins.connect(fig, tooltip)\n",
    "with open(f'tm-results/plots/nmf_{n_components}.html','w') as f:\n",
    "    mpld3.save_html(fig,f)\n",
    "    \n",
    "# produce a legend with a cross section of colours from the scatter\n",
    "handles, labels = scatter.legend_elements(prop=\"colors\", alpha=0.6)\n",
    "legend2 = ax.legend(\n",
    "    handles, labels, \n",
    "    loc=\"lower right\", \n",
    "    #bbox_to_anchor=(1.2,0.8),\n",
    "    title=\"Pub Year\"\n",
    ")\n",
    "\n",
    "plt.savefig(f'tm-results/plots/nmf_{n_components}.pdf', bbox_inches=\"tight\")\n",
    "plt.savefig(f'tm-results/plots/nmf_{n_components}.png', dpi=150, bbox_inches=\"tight\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can also try some other methods for topic modelling, see below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf = NMF(\n",
    "    n_components=n_components,\n",
    "    random_state=1,\n",
    "    beta_loss=\"kullback-leibler\",\n",
    "    solver=\"mu\",\n",
    "    max_iter=1000,\n",
    "    alpha=0.1,\n",
    "    l1_ratio=0.5,\n",
    ").fit(tfidf)\n",
    "\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "plot_top_words(\n",
    "    nmf,\n",
    "    tfidf_feature_names,\n",
    "    n_top_words,\n",
    "    \"Topics in NMF model (generalized Kullback-Leibler divergence)\",\n",
    "    n_components\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(\n",
    "    n_components=n_components,\n",
    "    max_iter=5,\n",
    "    learning_method=\"online\",\n",
    "    learning_offset=50.0,\n",
    "    random_state=0,\n",
    ")\n",
    "lda.fit(tf)\n",
    "\n",
    "tf_feature_names = tf_vectorizer.get_feature_names_out()\n",
    "plot_top_words(lda, tf_feature_names, n_top_words, \"Topics in LDA model\", n_components)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.9 (py39)",
   "language": "python",
   "name": "py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
